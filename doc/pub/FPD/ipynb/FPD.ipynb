{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6ce86c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html FPD.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Kvanteteknologi og kunstig intelligens -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4cf46a",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Kvanteteknologi og kunstig intelligens\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway\n",
    "\n",
    "Date: **Faglig pedagogisk dag, 31 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8095c9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Kort oppsumering\n",
    "Kvanteteknologi og kunstig intelligens er teknologier som vil kunne\n",
    "revolusjonere måten vi jobber og lever på og er forventa å kunne gi\n",
    "store fordeler for vitenskapelig og teknologisk utvikling, og vil\n",
    "sannsynligvis påvirke store og/eller alle deler av framtidas samfunn.\n",
    "Foredraget her vil ta for seg hvordan disse teknologiene vil påvirke\n",
    "naturfaglig og teknologisk forskning og undervisning, og hvorfor det\n",
    "er så viktig å forstå mulighetene og begrensningene.\n",
    "\n",
    "Lysark finner du her  <https://github.com/mhjensenseminars/MachineLearningTalk/tree/master/doc/pub/FPD>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ef309",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AI/ML and some statements you may have heard (and what do they mean?)\n",
    "\n",
    "1. Fei-Fei Li on ImageNet: **map out the entire world of objects** ([The data that transformed AI research](https://cacm.acm.org/news/219702-the-data-that-transformed-ai-research-and-possibly-the-world/fulltext))\n",
    "\n",
    "2. Russell and Norvig in their popular textbook: **relevant to any intellectual task; it is truly a universal field** ([Artificial Intelligence, A modern approach](http://aima.cs.berkeley.edu/))\n",
    "\n",
    "3. Woody Bledsoe puts it more bluntly: **in the long run, AI is the only science** (quoted in Pamilla McCorduck, [Machines who think](https://www.pamelamccorduck.com/machines-who-think))\n",
    "\n",
    "If you wish to have a critical read on AI/ML from a societal point of view, see [Kate Crawford's recent text Atlas of AI](https://www.katecrawford.net/).\n",
    "\n",
    "**Here: with AI/ML we intend a collection of machine learning methods with an emphasis on statistical learning and data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35799f73",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Curse of dimensionality\n",
    "\n",
    "<!-- dom:FIGURE: [figures/mbpfig2.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/mbpfig2.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af3e18",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural network quantum states\n",
    "\n",
    "**Neural networks compactly represent complex high-dimensional functions.**\n",
    "\n",
    "Most quantum states of interest have distinctive features and intrinsic structures\n",
    "%FIGURE: [figures/mbpfig3.png, width=900 frac=1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094afd1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Machine learning. A simple perspective on the interface between ML and Physics\n",
    "\n",
    "<!-- dom:FIGURE: [figures/mlimage.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/mlimage.png\" width=\"800\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c2cd2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Types of machine learning\n",
    "\n",
    "The approaches to machine learning are many, but are often split into two main categories. \n",
    "In *supervised learning* we know the answer to a problem,\n",
    "and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*\n",
    "is a method for finding patterns and relationship in data sets without any prior knowledge of the system.\n",
    "\n",
    "An important  third category is  *reinforcement learning*. This is a paradigm \n",
    "of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, \n",
    "solely from rewards and punishment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18768d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Main categories\n",
    "Another way to categorize machine learning tasks is to consider the desired output of a system.\n",
    "Some of the most common tasks are:\n",
    "\n",
    "  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.\n",
    "\n",
    "  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.\n",
    "\n",
    "  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632f304",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The plethora  of machine learning algorithms/methods\n",
    "\n",
    "1. Deep learning: Neural Networks (NN), Convolutional NN, Recurrent NN, Boltzmann machines, autoencoders and variational autoencoders  and generative adversarial networks, stable diffusion and many more generative models\n",
    "\n",
    "2. Bayesian statistics and Bayesian Machine Learning, Bayesian experimental design, Bayesian Regression models, Bayesian neural networks, Gaussian processes and much more\n",
    "\n",
    "3. Dimensionality reduction (Principal component analysis), Clustering Methods and more\n",
    "\n",
    "4. Ensemble Methods, Random forests, bagging and voting methods, gradient boosting approaches \n",
    "\n",
    "5. Linear and logistic regression, Kernel methods, support vector machines and more\n",
    "\n",
    "6. Reinforcement Learning; Transfer Learning and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c050459",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example of discriminative modeling, [taken from Generative Deep Learning by David Foster](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/standarddeeplearning.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/standarddeeplearning.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1aa9e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example of generative modeling, [taken from Generative Deep Learning by David Foster](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/generativelearning.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/generativelearning.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d72fcf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Taxonomy of generative deep learning, [taken from Generative Deep Learning by David Foster](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/generativemodels.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/generativemodels.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75f13d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Good books with hands-on material and codes\n",
    "* [Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html)\n",
    "\n",
    "* [David Foster, Generative Deep Learning with TensorFlow](https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html)\n",
    "\n",
    "* [Babcock and Gavras, Generative AI with Python and TensorFlow 2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)\n",
    "\n",
    "All three books have GitHub sites from where  one can download all codes. A good and more general text (2016)\n",
    "is Goodfellow, Bengio and Courville, [Deep Learning](https://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b4c46",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More references\n",
    "\n",
    "**Reading on diffusion models.**\n",
    "\n",
    "1. A central paper is the one by Sohl-Dickstein et al, Deep Unsupervised Learning using Nonequilibrium Thermodynamics, <https://arxiv.org/abs/1503.03585>\n",
    "\n",
    "2. See also Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho, Variational Diffusion Models, <https://arxiv.org/abs/2107.00630>\n",
    "\n",
    "   \n",
    "\n",
    "**and VAEs.**\n",
    "\n",
    "1. An Introduction to Variational Autoencoders, by Kingma and Welling, see <https://arxiv.org/abs/1906.02691>\n",
    "\n",
    "**And two Nobel prizes this year. Physics and Chemistry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d943cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What are the basic Machine Learning ingredients?\n",
    "Almost every problem in ML and data science starts with the same ingredients:\n",
    "* The dataset $\\boldsymbol{x}$ (could be some observable quantity of the system we are studying)\n",
    "\n",
    "* A model which is a function of a set of parameters $\\boldsymbol{\\alpha}$ that relates to the dataset, say a likelihood  function $p(\\boldsymbol{x}\\vert \\boldsymbol{\\alpha})$ or just a simple model $f(\\boldsymbol{\\alpha})$\n",
    "\n",
    "* A so-called **loss/cost/risk** function $\\mathcal{C} (\\boldsymbol{x}, f(\\boldsymbol{\\alpha}))$ which allows us to decide how well our model represents the dataset. \n",
    "\n",
    "We seek to minimize the function $\\mathcal{C} (\\boldsymbol{x}, f(\\boldsymbol{\\alpha}))$ by finding the parameter values which minimize $\\mathcal{C}$. This leads to  various minimization algorithms. It may surprise many, but at the heart of all machine learning algortihms there is an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37368ee3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Low-level machine learning, the family of ordinary least squares methods\n",
    "\n",
    "Our data which we want to apply a machine learning method on, consist\n",
    "of a set of inputs $\\boldsymbol{x}^T=[x_0,x_1,x_2,\\dots,x_{n-1}]$ and the\n",
    "outputs we want to model $\\boldsymbol{y}^T=[y_0,y_1,y_2,\\dots,y_{n-1}]$.\n",
    "We assume  that the output data can be represented (for a regression case) by a continuous function $f$\n",
    "through"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f365e71",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y}=f(\\boldsymbol{x})+\\boldsymbol{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d7442",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the equations\n",
    "\n",
    "In linear regression we approximate the unknown function with another\n",
    "continuous function $\\tilde{\\boldsymbol{y}}(\\boldsymbol{x})$ which depends linearly on\n",
    "some unknown parameters\n",
    "$\\boldsymbol{\\theta}^T=[\\theta_0,\\theta_1,\\theta_2,\\dots,\\theta_{p-1}]$.\n",
    "\n",
    "The input data can be organized in terms of a so-called design matrix \n",
    "with an approximating function $\\boldsymbol{\\tilde{y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14de16",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\tilde{y}}= \\boldsymbol{X}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477fcc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The objective/cost/loss function\n",
    "\n",
    "The  simplest approach is the mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61738f20",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\Theta})=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}}\\right)\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe7626",
   "metadata": {
    "editable": true
   },
   "source": [
    "or using the matrix $\\boldsymbol{X}$ and in a more compact matrix-vector notation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47d6ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\Theta})=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031926ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "This function represents one of many possible ways to define the so-called cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24826a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training solution\n",
    "\n",
    "Optimizing with respect to the unknown parameters $\\theta_j$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043078d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{y} = \\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa0617",
   "metadata": {
    "editable": true
   },
   "source": [
    "and if the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ is invertible we have the optimal values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee38f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} =\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcfae9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "We say we 'learn' the unknown parameters $\\boldsymbol{\\theta}$ from the last equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac927b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Selected references\n",
    "* [Mehta et al.](https://arxiv.org/abs/1803.08823) and [Physics Reports (2019)](https://www.sciencedirect.com/science/article/pii/S0370157319300766?via%3Dihub).\n",
    "\n",
    "* [Machine Learning and the Physical Sciences by Carleo et al](https://link.aps.org/doi/10.1103/RevModPhys.91.045002)\n",
    "\n",
    "* [Artificial Intelligence and Machine Learning in Nuclear Physics, Amber Boehnlein et al., Reviews Modern of Physics 94, 031003 (2022)](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.94.031003) \n",
    "\n",
    "* [Particle Data Group summary on ML methods](https://pdg.lbl.gov/2021/reviews/rpp2021-rev-machine-learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66fd4d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Scientific Machine Learning\n",
    "\n",
    "An important and emerging field is what has been dubbed as scientific ML, see the article by Deiana et al, Applications and Techniques for Fast Machine Learning in Science, Big Data **5**, 787421 (2022) <https://doi.org/10.3389/fdata.2022.787421>\n",
    "\n",
    "The authors discuss applications and techniques for fast machine\n",
    "learning (ML) in science - the concept of integrating power ML\n",
    "methods into the real-time experimental data processing loop to\n",
    "accelerate scientific discovery. The report covers three main areas\n",
    "\n",
    "1. applications for fast ML across a number of scientific domains;\n",
    "\n",
    "2. techniques for training and implementing performant and resource-efficient ML algorithms;\n",
    "\n",
    "3. and computing architectures, platforms, and technologies for deploying these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d38a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ML for detectors\n",
    "\n",
    "<!-- dom:FIGURE: [figures/detectors.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/detectors.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6641afa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Physics driven Machine Learning\n",
    "\n",
    "Another hot topic is what has loosely been dubbed **Physics-driven deep learning**. See the recent work on [Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators, Nature Machine Learning, vol 3, 218 (2021)](https://www.nature.com/articles/s42256-021-00302-5).\n",
    "\n",
    "**From their abstract.**\n",
    "\n",
    "A less known but powerful result is that an NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. ...  We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2703a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Argon-46 by Solli et al., NIMA 1010, 165461 (2021)\n",
    "\n",
    "Representations of two events from the\n",
    "Argon-46 experiment. Each row is one event in two projections,\n",
    "where the color intensity of each point indicates higher charge values\n",
    "recorded by the detector. The bottom row illustrates a carbon event with\n",
    "a large fraction of noise, while the top row shows a proton event\n",
    "almost free of noise.\n",
    "\n",
    "<!-- dom:FIGURE: [figures/examples_raw.png, width=500 frac=0.6] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/examples_raw.png\" width=\"500\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121273b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neutron star structure\n",
    "\n",
    "<!-- dom:FIGURE: [figures/mbpfig5.png, width=900 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/mbpfig5.png\" width=\"900\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900257f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## [Dilute neutron star matter from neural-network quantum states by Fore et al, Physical Review Research 5, 033062 (2023)](https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.5.033062) at density $\\rho=0.04$ fm$^{-3}$\n",
    "\n",
    "<!-- dom:FIGURE: [figures/nmatter.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/nmatter.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf6e30",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Education and advanced training\n",
    "\n",
    "1. Outreach and communication on quantum technologies and AI, explaining quantum technologies and AI to a broader audience\n",
    "\n",
    "2. Research on education in AI and QT. How are these topics best communicated and implemented in different enviroments, from  high school education to universities and to a broader audience, including external partners\n",
    "\n",
    "3. QAI-TALENT, Education and knowledge transfer through the development of advanced educational programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de552899",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Education, Quantum and AI/Machine Learning\n",
    "\n",
    "At the university of Oslo we have now established several educational\n",
    "programs in AI and QTs and quantum science. These programs span the\n",
    "whole spectrum from beginners courses to advanced training and\n",
    "education tailored to the specific needs of the participants.\n",
    "\n",
    "Furthermore, through research done at the center for Computing in\n",
    "Science Education and the physics education research group at the\n",
    "department of physics of the university of Oslo, we have over the\n",
    "years developed knowledge and insights on how to teach central\n",
    "concepts in quantum science as well as developing computational\n",
    "literacy and understanding of central algorithms applied to scientific\n",
    "problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b2199",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Courses and study programs\n",
    "\n",
    "1. **New study direction on Quantum technology** in Bachelor program Physics and Astronomy, starts Fall 2024. Three new courses:\n",
    "\n",
    "  * FYS1400 Introduction to Quantum Technologies\n",
    "\n",
    "  * FYS3405/4405 Quantum Materials\n",
    "\n",
    "  * FYS3415/4415 Quantum Computing\n",
    "\n",
    "2. **Developed Master of Science program on Computational Science**, started fall  2018 and many students here work on quantum computing and machine learning\n",
    "\n",
    "3. Developed courses on machine learning, from basic to advanced ones\n",
    "\n",
    "4. Developed advanced course on quantum computing and quantum machine learning, MAT3420, MAT4430/9430, FYS5419/9419\n",
    "\n",
    "5. New study directions in Master of Science in Physics and Computational Science on Quantum technologies and more. Start fall 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b9fa2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Content of courses we offer\n",
    "1. Quantum Information theory\n",
    "\n",
    "2. From Classical Information theory to Quantum Information theory\n",
    "\n",
    "3. Classical and Quantum Laboratory \n",
    "\n",
    "4. Discipline-Based Quantum Mechanics \n",
    "\n",
    "5. Quantum algorithms, computing, software and hardware\n",
    "\n",
    "6. Several  machine learning/AI courses, at all levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d434e04",
   "metadata": {
    "editable": true
   },
   "source": [
    "## QAI-TALENT: Education for a broader audience\n",
    "\n",
    "We have yearslong experience (with research based evidence on what works or not) in developing intensive training courses on ML/AI and QT.\n",
    "We  plan to develop an educational activity on quantum science and AI, \\textbf{QAI TALENT}\n",
    "(TALENT=Training and Advanced Lectures in EmergiNg Technologies) offering\n",
    "\n",
    "1. Intensive short courses on selected topics (which can lead to credits and certificates)\n",
    "\n",
    "2. Certificates of expertise with modules that can add up to one year of credits or more.\n",
    "\n",
    "3. Possibilities of adding up to a master specialization in quantum science/technologies and/or AI/ML\n",
    "\n",
    "4. Common educational projects and supervision of students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136debbb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Research directions, not exhaustive\n",
    "\n",
    "1. **Theory and experiments for quantum sensors**, from standard many-body theories, via machine learning to quantum computing. Close collaboration with Norwegian industry\n",
    "\n",
    "2. **Theory and experiments for quantum computing and quantum information theory**\n",
    "\n",
    "3. **Fundamental studies (theory and experiment) of quantum mechanics**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
