\documentclass{beamer}

\usetheme{Madrid}
\usepackage{amsmath,amssymb,physics}
\usepackage{bm}

\title{Backflow in Neural Quantum States}
\subtitle{Slater--Jastrow--Backflow Wavefunctions and Links to Transformers}
\author{MHJ }
\date{}

\begin{document}

%-------------------------------------------------
\begin{frame}
\titlepage
\end{frame}

%-------------------------------------------------
\begin{frame}{Outline}
\begin{enumerate}
    \item Slater--Jastrow wavefunctions (baseline for fermions)
    \item Backflow: explicit coordinate/orbital transformations
    \item Slater--Jastrow--Backflow (SJB) equations (continuum and lattice)
    \item Why backflow improves nodal structure and correlations
    \item Relation to Transformer-style architectures
\end{enumerate}
\end{frame}

%=================================================
\section{Slater--Jastrow baseline}

\begin{frame}{Slater--Jastrow (SJ) variational ansatz}
For $N$ fermions with coordinates $\bm{R}=(\bm{r}_1,\dots,\bm{r}_N)$,
a standard correlated ansatz is the Slater--Jastrow form
\[
\Psi_{\mathrm{SJ}}(\bm{R})
=
\det\!\big[\phi_k(\bm{r}_i)\big]\;
e^{J(\bm{R})}.
\]

\begin{itemize}
    \item $\det[\phi_k(\bm{r}_i)]$ enforces antisymmetry and defines the nodal surface.
    \item $e^{J(\bm{R})}$ (Jastrow factor) captures symmetric correlations (often pairwise).
\end{itemize}

A common two-body Jastrow form:
\[
J(\bm{R}) = \sum_{i<j} u(r_{ij}),\qquad r_{ij}=\abs{\bm{r}_i-\bm{r}_j}.
\]
\end{frame}

\begin{frame}{Limitations of Slater--Jastrow}
\begin{itemize}
    \item The Jastrow factor improves correlation energy but \textbf{does not change the nodes}:
    \[
    \Psi_{\mathrm{SJ}}(\bm{R})=0 \iff \det[\phi_k(\bm{r}_i)]=0.
    \]
    \item For strongly correlated fermions, the \textbf{nodal surface} is often the main source of error.
    \item Backflow is designed specifically to introduce flexible, configuration-dependent nodal structure.
\end{itemize}
\end{frame}

%=================================================
\section{Backflow: explicit equations}

\begin{frame}{Backflow: configuration-dependent coordinates}
Backflow replaces bare coordinates $\bm{r}_i$ by \emph{effective} coordinates
\[
\tilde{\bm{r}}_i = \bm{r}_i + \bm{\xi}_i(\bm{R}),
\]
where the backflow displacement $\bm{\xi}_i$ depends on the full configuration.

A classical (pairwise) backflow form:
\[
\bm{\xi}_i(\bm{R}) = \sum_{j\neq i} \eta(r_{ij})\,(\bm{r}_i-\bm{r}_j),
\]
with a scalar backflow function $\eta(\cdot)$ (often parameterized).
\end{frame}

\begin{frame}{Slater--Jastrow--Backflow (SJB) ansatz}
The Slater determinant is evaluated at backflow coordinates:
\[
\Psi_{\mathrm{SJB}}(\bm{R})
=
\det\!\big[\phi_k(\tilde{\bm{r}}_i)\big]\;
e^{J(\bm{R})}.
\]

\begin{itemize}
    \item Backflow alters the nodes:
    \[
    \Psi_{\mathrm{SJB}}(\bm{R})=0 \iff \det[\phi_k(\tilde{\bm{r}}_i(\bm{R}))]=0,
    \]
    so the nodal surface becomes an implicit, many-body object.
    \item $J(\bm{R})$ still captures symmetric correlation and cusp conditions.
\end{itemize}
\end{frame}

\begin{frame}{Backflow as orbital deformation (equivalent view)}
Instead of moving coordinates, one may view backflow as making orbitals
configuration-dependent:
\[
\phi_k(\bm{r}_i)\quad \longrightarrow \quad
\phi_k\big(\bm{r}_i; \bm{R}\big)
:= \phi_k\big(\tilde{\bm{r}}_i(\bm{R})\big).
\]

Thus, each particle experiences an \emph{effective environment-dependent orbital}
set by the positions of all other particles.
\end{frame}

%=================================================
\section{Lattice / second-quantized analogues}

\begin{frame}{Lattice analogue: backflow features for occupations}
For lattice fermions with occupations $\bm{n}=(n_1,\dots,n_L)$,
a backflow-like construction introduces \emph{configuration-dependent features}
\[
\tilde{h}_i(\bm{n}) = h_i + \sum_{j\neq i} f_{ij}\,\Phi(n_i,n_j,\bm{n}),
\]
where $\tilde{h}_i$ plays the role of an effective site feature/embedding.

A determinant-based ansatz may then use effective orbitals
\[
\Psi(\bm{n}) \propto \det\!\big[ \varphi_k(\tilde{h}_i(\bm{n})) \big]\times e^{J(\bm{n})},
\]
with a Jastrow-like factor $J(\bm{n})$ capturing density--density correlations.
\end{frame}

\begin{frame}{Neural backflow: learned displacements or embeddings}
In NQS practice, backflow is often \textbf{neural}:
\[
\bm{\xi}_i(\bm{R}) = \mathcal{N}_\theta\!\left(i;\,\bm{R}\right),
\]
with $\mathcal{N}_\theta$ a symmetry-respecting neural network (e.g.\ equivariant).

Then
\[
\tilde{\bm{r}}_i = \bm{r}_i + \mathcal{N}_\theta(i;\bm{R}),
\qquad
\Psi_{\mathrm{SJB}}(\bm{R})
=
\det[\phi_k(\tilde{\bm{r}}_i)]\,e^{J(\bm{R})}.
\]

This generalizes pairwise backflow to \emph{collective} many-body backflow.
\end{frame}

%=================================================
\section{Relation to Transformers}

\begin{frame}{Transformer view: tokens, context, and backflow}
A Transformer builds \emph{context-dependent} representations:
\[
\text{token embedding } \bm{e}_i
\quad \longrightarrow \quad
\text{contextual embedding } \bm{h}_i(\{\bm{e}_j\}_{j=1}^N).
\]

Backflow is analogous:
\[
\bm{r}_i \quad \longrightarrow \quad
\tilde{\bm{r}}_i(\bm{R})=\bm{r}_i+\bm{\xi}_i(\bm{R}),
\]
i.e.\ each particle coordinate (or feature) becomes \textbf{contextual},
conditioned on the entire configuration.
\end{frame}

\begin{frame}{Self-attention as a backflow-like map (schematic)}
In a Transformer layer, one often has
\[
\bm{h}_i = \bm{e}_i + \sum_{j} \alpha_{ij}\, \bm{V}\bm{e}_j,
\qquad
\alpha_{ij}=\mathrm{softmax}_j\!\left(\frac{(\bm{Q}\bm{e}_i)\cdot(\bm{K}\bm{e}_j)}{\sqrt{d}}\right).
\]

Compare with pairwise backflow:
\[
\tilde{\bm{r}}_i = \bm{r}_i + \sum_{j\neq i} \eta(r_{ij})\,(\bm{r}_i-\bm{r}_j).
\]

\textbf{Analogy:}
\begin{itemize}
  \item weights $\alpha_{ij}$ or $\eta(r_{ij})$ quantify \emph{influence} of $j$ on $i$,
  \item both produce nonlocal, permutation-symmetric updates of per-particle features.
\end{itemize}
\end{frame}

\begin{frame}{Why Transformer-style backflow is powerful for NQS}
\begin{itemize}
  \item Long-range correlations are captured naturally via attention over all particles.
  \item Backflow/attention directly produces \textbf{configuration-dependent nodes} (fermions).
  \item Parameter-efficient: global structure can be learned with fewer parameters than very deep MLPs.
  \item Symmetry handling: permutation symmetry and (with equivariance) rotational symmetry can be enforced.
\end{itemize}

\vspace{0.3cm}
\textbf{Takeaway:} backflow is the physics analogue of contextual embeddings; Transformers are
a natural modern architecture to implement neural backflow maps.
\end{frame}

%=================================================
\section{Summary}

\begin{frame}{Summary}
\begin{itemize}
  \item Slater--Jastrow:
  \[
  \Psi_{\mathrm{SJ}}(\bm{R})=\det[\phi_k(\bm{r}_i)]\,e^{J(\bm{R})}
  \]
  captures correlations but leaves nodes fixed.
  \item Backflow introduces effective coordinates
  \[
  \tilde{\bm{r}}_i=\bm{r}_i+\bm{\xi}_i(\bm{R})
  \]
  and yields Slater--Jastrow--Backflow:
  \[
  \Psi_{\mathrm{SJB}}(\bm{R})=\det[\phi_k(\tilde{\bm{r}}_i)]\,e^{J(\bm{R})}.
  \]
  \item Backflow $\leftrightarrow$ contextualization: conceptually aligned with Transformer self-attention.
\end{itemize}
\end{frame}


%=================================================
\section{What are barren plateaus?}

\begin{frame}{Barren plateaus: definition}
A \textbf{barren plateau} is a region of parameter space where
\[
\mathbb{E}\!\left[\partial_{\theta_i} \mathcal{L}\right] = 0,
\qquad
\mathrm{Var}\!\left[\partial_{\theta_i} \mathcal{L}\right]
\sim e^{-cN},
\]
with $N$ the system size.

\vspace{0.3cm}

Consequences:
\begin{itemize}
  \item Gradients vanish exponentially with system size
  \item Optimization becomes exponentially hard
  \item Training is dominated by noise
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Physical origin of barren plateaus}
Barren plateaus arise when the variational state explores an
\emph{effectively random} region of Hilbert space:
\begin{itemize}
  \item High expressivity $\rightarrow$ states resemble Haar-random vectors
  \item Observables concentrate around their mean (concentration of measure)
\end{itemize}

\vspace{0.3cm}

This phenomenon is generic to:
\begin{itemize}
  \item deep random quantum circuits,
  \item unstructured variational ans\"atze,
  \item overly flexible neural wavefunctions without physical bias.
\end{itemize}
\end{frame}

%=================================================
\section{Barren plateaus in variational quantum algorithms}

\begin{frame}{Barren plateaus in VQE and QNNs}
In VQE and circuit-based QNNs:
\begin{itemize}
  \item deep parameterized circuits form approximate unitary designs,
  \item gradients of the energy concentrate near zero,
  \item expressivity grows faster than trainability.
\end{itemize}

\vspace{0.3cm}

Similarly, in NQS:
\begin{itemize}
  \item highly expressive networks may sample chaotic regions of state space,
  \item energy landscapes become flat almost everywhere.
\end{itemize}
\end{frame}

%=================================================
\section{Why structure matters}

\begin{frame}{Key insight: structure prevents barren plateaus}
Barren plateaus are \emph{not inevitable}. They are avoided by:
\begin{itemize}
  \item locality,
  \item symmetry constraints,
  \item physically motivated parameterizations.
\end{itemize}

\vspace{0.3cm}

\textbf{Principle:}
\[
\text{Expressivity} \quad \textbf{must be balanced with} \quad \text{inductive bias}.
\]
\end{frame}

%=================================================
\section{Backflow as a mitigation strategy}

\begin{frame}{Backflow suppresses barren plateaus}
Backflow modifies the wavefunction as
\[
\bm{x}_i \rightarrow \tilde{\bm{x}}_i(\bm{x}_1,\dots,\bm{x}_N),
\]
introducing correlations while preserving structure.

\vspace{0.3cm}

Why this helps:
\begin{itemize}
  \item gradients act on \emph{collective} coordinates,
  \item nodal surfaces evolve smoothly,
  \item the state remains close to physically relevant manifolds.
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Backflow vs unstructured expressivity}
\begin{center}
\begin{tabular}{lcc}
\hline
 & \textbf{Unstructured NN} & \textbf{Backflow NQS} \\
\hline
Symmetry-aware & No & Yes \\
Locality & Weak & Strong \\
Gradient scaling & Exponential decay & Polynomial / stable \\
Physical interpretability & Low & High \\
\hline
\end{tabular}
\end{center}

\vspace{0.3cm}

Backflow provides \emph{targeted expressivity} instead of random expressivity.
\end{frame}

%=================================================
\section{CTNN backflow and optimization}

\begin{frame}{Why CTNN backflow is especially effective}
CTNN backflow uses convolutional, translation-invariant maps:
\[
\tilde h_i = h_i + \sum_\delta \mathcal{C}_\theta(\delta)\, n_{i+\delta}.
\]

Advantages:
\begin{itemize}
  \item gradients scale with correlation length, not system size,
  \item parameter sharing stabilizes optimization,
  \item respects lattice symmetries by construction.
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Optimization landscape with CTNN backflow}
Empirically and theoretically:
\begin{itemize}
  \item gradients remain finite as $N$ increases,
  \item training converges with shallow depth,
  \item fewer parameters achieve the same accuracy.
\end{itemize}

\vspace{0.3cm}

CTNN backflow avoids exploring chaotic regions of Hilbert space, keeping the
state in a \emph{low-complexity manifold}.
\end{frame}

%=================================================
\section{Relation to Transformers}

\begin{frame}{Transformers: expressivity vs trainability}
Transformers introduce global context via attention:
\[
\bm{h}_i = \bm{e}_i + \sum_j \alpha_{ij}\bm{V}\bm{e}_j.
\]

\begin{itemize}
  \item High expressivity
  \item Risk of barren plateaus if unregularized
  \item Requires careful initialization and symmetry handling
\end{itemize}
\end{frame}

%-------------------------------------------------
\begin{frame}{Backflow as physics-informed contextualization}
Backflow can be viewed as:
\begin{itemize}
  \item a constrained attention mechanism,
  \item operating on relative coordinates or occupations,
  \item with locality and symmetry hard-coded.
\end{itemize}

\vspace{0.3cm}

\textbf{Key difference:}
\[
\text{Backflow} \Rightarrow \text{context with physics priors}.
\]
\end{frame}

%=================================================
\section{Practical optimization strategies}

\begin{frame}{Best practices to avoid barren plateaus}
\begin{itemize}
  \item Use shallow, structured architectures
  \item Introduce backflow before increasing depth
  \item Enforce symmetries explicitly
  \item Optimize incrementally (curriculum learning)
  \item Monitor gradient variance, not just loss
\end{itemize}
\end{frame}


\end{document}
